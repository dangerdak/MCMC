%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Imperial College
\documentclass[a4paper,11pt,twoside]{article}
\usepackage[left=2.5cm,right=2cm,top=2cm,bottom=2cm]{geometry}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Paragraph
\usepackage[parfill]{parskip}

% Images
\usepackage{graphicx} 
\usepackage{caption}

% URLs
\usepackage{hyperref}

% Maths
\usepackage{amsmath}
\usepackage{xfrac}

% Clever referencing
\usepackage{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document} 
\title{MCMC in Action: Applying the Metropolis-Hastings Algorithm to a Two
Dimensional Gaussian}

\date{\today} 
\author{Dakshina Scott} 
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract} 
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction: What is MCMC?} 
\subsection{Monte Carlo Methods, inc Rejection Sampling}

\subsection{Markov Chains} 
\subsection{Metropolis-Hastings Algorithm}
\subsection{Examples of Applications}

\section{One-Dimensional Posterior} 
The aim of the first part of this project was to find the posterior
distribution for a quantity $\theta$, for the case of a Gaussian prior and a
Gaussian likelihood. With our knowledge
of the distribution thus far given by $p(\theta)$, Bayes theorem gives us a way
to update our knowledge given a new set of measurements, $x$: 
\begin{equation}
	\label{eq:bayes}
	p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)},
\end{equation} 
where $p(\theta|x)$ is the posterior, our updated knowledge,
$p(x|\theta)$ is the likelihood of the data - the probability of observing
those data as a function of $\theta$, $p(\theta)$ is the prior distribution,
i.e. our knowledge before the measurements, and $p(x)$ is a normalizing
constant.

Measurements were simulated by generating a set of
independent, Gaussian distributed variables, $\hat{x} = \{x_1, x_2, ...,
x_N\}$, representing observed values for $\theta$. In this
case we used that
\begin{equation}
	\label{eq:measurements}
	p(\theta) \sim \mathcal{N}(0.5, 0.1),
\end{equation}
in order to generate our results - i.e. that we have a population mean of 0.5,
and a population variance of 0.1. If the data $\hat{x}$ had come from an actual
experiment of course we would not know these two values.
The likelihood function for the data $\hat{x}$ is given by:

\begin{equation}
	\label{eq:rawlikelihood}
	\mathcal{L}(\theta) = p(\hat{\mathbf{x}}|\bar{x},\sigma) = \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2}\frac{(\theta - \hat{x}_i)^2}{\sigma^2}\right),
\end{equation}

from which it can be shown that:
\begin{equation}
	\label{eq:likelihood}
	\mathcal{L}(\theta) = L_0\exp\left(-\frac{1}{2}\frac{(\theta - \bar{x})^2}{\sfrac{\sigma^2}{N}}\right),
\end{equation}
where $L_0$ is a constant (see \cref{sec:likelihood}).

Assuming the following prior distribution:
\begin{equation}
	\label{eq:prior}
	p(\theta) =  \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}\frac{\theta^2}{\Sigma^2}\right),
\end{equation}
with $\Sigma = 1$, Bayes theorem can be used to find the posterior distribution.
\subsection{Analytical Solution}
\subsection{Rejection Sampling} 
\subsection{Metropolis-Hastings Algorithm} 
\subsection{Analytical vs Rejection vs MH}

\section{Two-Dimensional} 
\subsection{Analytical Solution}
\subsection{Metropolis-Hastings} 
\subsection{Analytical vs MH}

\section{Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix 
\label{appendix}

\section{One-Dimensional Likelihood Function}
\label{sec:likelihood}
The likelihood distribution for a set of $N$ measurements is given by the product of the likelihood for each measurement:
\begin{align*}
	\mathcal{L}(\theta) &= \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2}\frac{(\theta - \hat{x}_i)^2}{\sigma^2}\right)
	\\ &= \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^N \exp\left(-\frac{1}{2}\sum_{i=1}^{N}(\frac{\theta - \hat{x}_i)^2}{\sigma^2}\right).
\end{align*}
Taking, for now, just the exponential, and using that $\bar{x} = \frac{1}{N} \sum_i \bar{x}_i \Rightarrow \sum_i \hat{x}_i = N \bar{x}$:
\begin{align*}
	-\frac{1}{2\sigma^2}\sum_i^N(\theta - \hat{x})^2 &= -\frac{1}{2\sigma^2}(N\theta^2 - 2\theta \sum_i^N\hat{x}_i + \sum_i^N \hat{x}_i^2) 
	\\ &= -\frac{1}{2\sigma^2}(N\theta^2 - 2\theta N \bar{x} + \sum_i^N \hat{x}_i^2) 
	\\ &= -\frac{N}{2\sigma^2} (\theta^2 - 2\theta \bar{x} [ + \bar{x}^2 - \bar{x}^2]  + \frac{1}{N} \sum_i^N \hat{x}_i^2) 
	\\ &= -\frac{N}{2 \sigma^2} (\theta - \bar{x})^2 - \frac{N}{2 \sigma^2}(\frac{1}{N} \sum_{i=1}^{N} \hat{x}_i^2 - \bar{x}^2).
\end{align*}
So
\begin{align*}
	\mathcal{L}(\theta) &= \left( \frac{1}{\sqrt{2\pi}} \right)^N \exp \left(- \frac{N}{2 \sigma^2}(\frac{1}{N} \sum_{i=1}^{N} \hat{x}_i^2 - \bar{x}^2)\right) \exp \left(-\frac{N}{2 \sigma^2} (\theta - \bar{x})^2 \right)
	\\ &= L_0 \exp \left( -\frac{N}{2 \sigma^2} (\theta - \bar{x})^2 \right),
\end{align*}
where $L_0 = \left( \frac{1}{\sqrt{2\pi}} \right)^N \exp \left(- \frac{N}{2 \sigma^2}(\frac{1}{N} \sum_i \hat{x}_i^2 - \bar{x}^2)\right)$.

\section{Computing the Posterior Probability for Theta}
Bayes theorem is given by:
\begin{align*}
	p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)}.
\end{align*}
Ignoring the normalization constant (as it is independent of $\theta$):
\begin{align*}
	p(\theta|x) \propto p(x|\theta)p(\theta).
\end{align*}
We have that
\begin{equation*}
	p(x|\theta) = L_0 \exp \left( -\frac{N}{2 \sigma^2} (\theta - \bar{x})^2 \right),
\end{equation*}
and
\begin{equation*}
	p(\theta) =  \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}\frac{\theta^2}{\Sigma^2}\right).
\end{equation*}

From these,
\begin{equation*}
	p(\theta|x) \propto \exp \left[ -\frac{1}{2} \left( \frac{(\theta - \bar{x})^2}{\sfrac{\sigma^2}{N}} +\frac{\theta^2}{\Sigma^2} \right) \right].
\end{equation*}

Taking just the exponent,
\begin{align*}
	\frac{(\theta - \bar{x})^2}{\sfrac{\sigma^2}{N}} + \frac{\theta^2}{\Sigma^2} &= \frac{N}{\Sigma^2\sigma^2} \left[\Sigma^2 (\theta - \bar{x})^2 + \frac{\sigma^2}{N} \theta^2 \right]
	\\ &= \left( \frac{1}{\Sigma^2} + \frac{N}{\sigma^2} \right) \left( \frac{1}{\sfrac{\sigma^2}{N} + \Sigma^2} \right) \left[ (\Sigma^2 + \frac{\sigma^2}{N})\theta^2 - 2\bar{x}\Sigma^2\theta + \Sigma^2\bar{x}^2 \right]
	\\ &= \left( \frac{1}{\Sigma^2} + \frac{N}{\sigma^2} \right) \left( \theta^2 - 2 \frac{\Sigma^2}{\sfrac{\sigma^2}{N} + \Sigma^2} \theta + \frac{\Sigma^2}{\sfrac{\sigma^2}{N} + \Sigma^2} \bar{x}^2 \right).
\end{align*}
The final term above is independent of $\theta$. As we are dealing with an exponent, we can subtract this term and add its square without losing proportionality. This allows us to complete the square so we have:
\begin{equation*}
	\left( \frac{1}{\Sigma^2} + \frac{N}{\sigma^2} \right) \left( \theta - \frac{\Sigma^2}{\sfrac{\sigma^2}{N} + \Sigma^2} \bar{x} \right)^2.
\end{equation*}

Putting this back inside the exponential we are left with:
\begin{equation*}
	p(\theta|x) \propto \exp \left[ -\frac{1}{2} \left( \frac{1}{\Sigma^2} + \frac{N}{\sigma^2} \right) \left( \theta - \frac{\Sigma^2}{\sfrac{\sigma^2}{N} + \Sigma^2} \bar{x} \right)^2 \right],
\end{equation*}

i.e. the posterior follows a Gaussian distribution with mean $\frac{\Sigma^2}{\sfrac{\sigma^2}{N} + \Sigma^2} \bar{x}$ and standard deviation $\left( \frac{1}{\Sigma^2} + \frac{N}{\sigma^2} \right)^{-\frac{1}{2}}$  

\section{onedcode} 
\label{sec:onedcode}

\section{twodcode}
\label{sec:twodcode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thebibliography}{10}


\end{thebibliography}

\end{document}
