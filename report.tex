%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Imperial College
\documentclass[a4paper,11pt,twoside]{article}
\usepackage[left=2.5cm,right=2cm,top=2cm,bottom=2cm]{geometry}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Paragraph
\usepackage[parfill]{parskip}

% Images
\usepackage{graphicx} 
\usepackage{caption}
\usepackage{subcaption}

% URLs
\usepackage{hyperref}

% Maths
\usepackage{amsmath}
\usepackage{xfrac}

% Clever referencing
\usepackage{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document} 
\title{MCMC in Action: Applying the Metropolis-Hastings Algorithm to a Two
Dimensional Gaussian}

\date{\today} 
\author{Dakshina Scott} 
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract} 
The Metropolis-Hastings algorithm was used to generate samples from a probability
distribution. The samples were used, via maximum likelihood estimates to
find Monte Carlo estimates for the parameters of the distribution.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background} 
Markov Chain Monte Carlo (MCMC) can be used to solve problems which would
otherwise not be solvable - such as intractable integrations or sampling from
complicated multivariate probability distributions.  

While the idea of Monte Carlo simulations has been
around for much longer, MCMC has flourished since the rise of computers allowed
much larger simulations. It was originally developed by Metropolis et al. at Los Alamos in
1953 to investigate the equation of state for substances consisting of
individual interacting molecules \cite{metropolis}.  

\subsection{Monte Carlo Methods \& Ordinary Monte Carlo}
Monte Carlo methods use random numbers to solve problems.  A Monte Carlo method
may be more narrowly defined as
"representing the solution of a problem as a parameter of a hypothetical
population, and using a random sequence of numbers to construct a sample of the
population, from which statistical estimates of the parameter can be found"
\cite{halton}.  

Ordinary Monte Carlo uses a sequence of random, independent identically distibuted
numbers are used in

A very simple example is a Monte Carlo estimate of $\pi$. Assume we have a
circle of radius one, contained exactly within a square ranging [-1, 1]. The
probability of random points from a uniform distribution within this range
landing in the circle is given by:
\begin{equation}
	P(inside) = \frac{\text{area of circle}}{\text{area of square}} =
	\frac{\pi}{4}.
\end{equation}
As there are only two possible outcomes for each simulation - a point lands
either inside or outside of the circle - a  population of N points can be
described by a binomial distribution in which a 'success' is a point landing within
the circle:
\begin{equation}
	I \sim \mathcal{B}(N, \theta)
\end{equation}
where $\theta = P(\text{inside})$, and $I$ is the number of successes.
Thus we have represented the solution of our problem as a parameter of a
hypothetical population, that is a binomial population with unknown probability
of success.
We can estimate $\theta$ using its maximum-likelihood estimate \cite{som}, based
on the number of success we observed in our random sampling:
\begin{equation}
	\theta = \frac{I}{N},
\end{equation}

So a Monte Carlo estimate for $\pi$ is given by
\begin{equation}
	\pi_{MC} = 4 \theta = 4 \frac{I}{N}.
\end{equation}

\subsection{Rejection Sampling}
Rejection sampling is a particular type of Monte Carlo method which can be used
if the target distribution can be evaluated, at least to within a normalization
constant. 

A random number $r$ is generated from some proposal distribution, $Q(\theta)$. A
corresponding random number is generated from a uniform distribution in the
range [0,$Q(\theta = r)$], representing a value on the y-axis. Both the
proposal distribution and the target distribution, $P(\theta|x)$, are
evaluated at this value. The probability of 'accepting' the sample is given by
$\frac{P(\theta = r|x)}{Q(\theta = r)}$ - in practice this is implemented by accepting the
sample if $y < P(\theta = r|x)$, and rejecting otherwise. The accepted points are
effectively a series of samples from the target distribution. From these
samples using the maximum-likelihood estimates for mean and variance gives the
Monte Carlo estimates for said quantities. It is important that $Q(\theta) >
P(\theta|x) \, \forall \, \theta$. 

\subsection{Markov Chains} 
\subsection{Metropolis-Hastings Algorithm}
\subsection{Examples of Applications}
\subsection{Analytical Solution}
In this case we have a simple gaussian prior and likelihood, and it can be shown
that the resulting posterior is also a gaussian (see \cref{sec:posterior}).
Because we know the form of the equation for a gaussian distribution, it can be
seen that the posterior mean and standard deviation are given by:
\begin{equation}
	\mu_{post} = \frac{\Sigma^2}{\sfrac{\sigma^2}{N} + \Sigma^2} \bar{x},
\end{equation}

\begin{equation}
	\sigma_{post} = \left( \frac{1}{\Sigma^2} + \frac{N}{\sigma^2} \right)^{-\frac{1}{2}}.
\end{equation}

This analytical solution gives us something to which our MCMC results can be
compared. However, in real world applications of MCMC, of course this would not
be available.

\subsection{Rejection Sampling} 
First a simple Monte Carlo method - rejection sampling - was applied to our toy
problem.
\begin{figure}[ht]
	\centering
	\begin{subfigure}[t]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{rejection.png}
		\caption{The histogram in green represents the distribution
			found by rejection sampling. The Monte Carlo mean and standard
			deviation are found to be $\mu_{MC} = 0.what$ and $\mu_{MC} = 0.0wat$.
			The blue line is the analytical distribution.}
		\label{fig:rejection}
	\end{subfigure}
	~
	\begin{subfigure}[t]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{recentlog.png}
		\caption{A log-plot makes it easier to see the discrepencies at
			the extremities of the plot. These are due to the finite
		domain over which samples are taken using this method.}
		\label{fig:rejlog}
	\end{subfigure}
\end{figure}

In \cref{fig:rejection}, the results appear to fit the analytical curve well.
However, plotting the logarithm of the results allows us to see clearly differences at
the edges of our Monte Carlo sample - this is because we can't take samples over
an infinite domain, and in this algorithm we must choose definite cut-off
points. The wider the domain the smaller the effect of this will be, but the
number of rejected candidate points will be larger. As there always has to be a
cut-off somewhere, this is an area where the Metropolis-Hastings algorithm will
be more effective (RANDOM WALK???). (AND ALSO ERROR - HOW DOES IT GO WITH MORE
DIMENSIONS??? - AND
EFFICIENCY???) (AND OTHER WAYS IT'S BETTER???).

\subsection{Metropolis-Hastings Algorithm} 
The Metropolis-Hastings algorithm was then applied to the same problem. 
\begin{figure}[ht]
	\centering
	\begin{subfigure}[t]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{metropolishastings.png}
		\caption{The histogram in green represents the distribution
			found by the Metropolis-Hastings algorithm. The Monte Carlo mean and standard
			deviation are found to be $\mu_{MC} = 0.what$ and $\mu_{MC} = 0.0wat$.
			The blue line is the analytical distribution.}
		\label{fig:metropolis}
	\end{subfigure}
	~
	\begin{subfigure}[t]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{recentlog.png}
		\caption{A log-plot of the Metropolis-Hastings results shows
		that the sample is more consistent with the analytical solution,
	as compared with \cref{fig:rejlog}.}
		\label{fig:mhlog}
	\end{subfigure}
\end{figure}

BURN-IN??? - INC MULTIPLE STARTS CONVERGING

PROPOSAL SIGMA/ACCEPTANCE RATE???

MC SIGMA INDEPENDENT OF PROPOSAL SIGMA
\subsection{Analytical vs Rejection vs MH}

\section{Two-Dimensional Target Distribution} 
\subsection{Analytical Solution}
Consider the linear model
\begin{equation}
	y = F\theta + \epsilon
\end{equation}
where y is a vector containing our data, $\theta$ is a vector of unkown
parameters, F is the design matrix and $\epsilon$ is a vector containing the
noise. If we assume that the noise is randomly gaussian distributed with zero
mean and zero correlation, then the likelihood function can be shown to take the form
\begin{equation}
	p(y|\theta) = \mathcal{L}_0 \exp\left[-\frac{1}{2}(\theta - \theta_0)^tL(\theta - \theta_0)\right],
\end{equation}
where $L$ is the likelihood fisher matrix, and $\mathcal{L}_1$ is a constant (see \cref{sec:2dlikelihood}).

??? what's theta_0?

If we then also say we have a Gaussian prior with zero mean and Fisher matrix $P$, i.e.
\begin{equation}
	p(\theta) = \frac{|P|^{\sfrac{1}{2}}}{(2\pi)^{\sfrac{n}{2}}} \exp \left[ \frac{1}{2} \theta^t P \theta \right],
\end{equation}

then using Bayes theorem the posterior can be shown to follow
\begin{equation}
	p(\theta|y) \propto \exp \left[ -\frac{1}{2} (\theta - \bar{\theta})^t \mathcal{F} (\theta - \bar{\theta}) \right],
\end{equation}
where $\mathcal{F} = L + P$ and $\bar{\theta} = \mathcal{F}^{-1}L\theta_0$ (see \cref{sec:2dposterior}).
From this it is clear that $\bar{\theta}$ is the posterior mean and
$\mathcal{F}$ is the posterior Fisher matrix, but only because both the prior
and the likelihood had the same (Gaussian) form - resulting in a Gaussian
posterior.

While these results apply to the multivariate case in general, here we
specialize to the bivariate case. Specifically we take a prior with Fisher matrix
	$ P = \begin{bmatrix}
		10^{-2} & 0 \\
		0 & 10^{-2}
	\end{bmatrix}, $
and a set of simulated data points with noise (see \cref{sec:2ddata}).
From this we find that the posterior mean, 
	$\bar{\theta} = \begin{bmatrix} 
		\theta_1 \\ 
		\theta_2 \end{bmatrix} = \begin{bmatrix} 
		-0.012 \\ 
	1.329 \end{bmatrix}$.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\textwidth]{2ddata.png}
	\caption{In blue are the simulated data with their associated
	error bars. The green line is the linear model $y = \theta_1 + \theta_2
	x$ with the parameters $\theta_1$ and $\theta_2$ found analytically.}
	\label{fig:model}
\end{figure}

\subsection{Metropolis-Hastings} 
The Metropolis-Hastings algorithm was then applied to this bivariate posterior
distribution. The program includes a preliminary run - this is done in order
to find a rough outline of the posterior distribution. From this the
orientation of the elliptical distribution can be found, which is used in order
to transform the distribution to a unit circle. Samples are taken from this
using Metropolis-Hastings, and then transformed back to the original
distribution. This makes the choice of proposal distribution simpler and the
algorithm more efficient - it is easiest to choose the proposal distribution
such that the standard deviation is specified in the x and y directions.
However if the ellipse of our desired distribution is not aligned with the axes
then this will reduce the efficiency of the mixing. Transforming the ellipse
not only aligns it to the axes (and thus to the proposal distribution) but also
means we can use the same proposal standard devation in along each axis.
\begin{figure}[ht]
	\centering
	\begin{subfigure}[t]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{rot.png}
		\caption{Metropolis-Hastings samples as they appear when
		transformed. The proposal distribution standard devation used
		is shown in black.}
		\label{fig:rot}
	\end{subfigure}
	~
	\begin{subfigure}[t]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{unrot.png}
		\caption{Metropolis-Hastings samples transformed back to
		an ellipse. The transformed proposal standard deviation is shown
		in black, and an example of the sort of standard deviation which would
		have been used if the ellipse hadn't been rotated is shown in dashed red. ??? }
		\label{fig:rejlog}
	\end{subfigure}
\end{figure}


\subsection{Analytical vs MH}

\section{Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix 
\label{appendix}

\section{One-Dimensional Likelihood Function}
\label{sec:likelihood}
The likelihood distribution for a set of $N$ measurements is given by the product of the likelihood for each measurement:
\begin{align*}
	\mathcal{L}(\theta) &= \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2}\frac{(\theta - \hat{x}_i)^2}{\sigma^2}\right)
	\\ &= \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^N \exp\left(-\frac{1}{2}\sum_{i=1}^{N}(\frac{\theta - \hat{x}_i)^2}{\sigma^2}\right).
\end{align*}
Taking, for now, just the exponent, and using that $\bar{x} = \frac{1}{N} \sum_i \bar{x}_i \Rightarrow \sum_i \hat{x}_i = N \bar{x}$:
\begin{align*}
	-\frac{1}{2\sigma^2}\sum_i^N(\theta - \hat{x})^2 &= -\frac{1}{2\sigma^2}(N\theta^2 - 2\theta \sum_i^N\hat{x}_i + \sum_i^N \hat{x}_i^2) 
	\\ &= -\frac{1}{2\sigma^2}(N\theta^2 - 2\theta N \bar{x} + \sum_i^N \hat{x}_i^2) 
	\\ &= -\frac{N}{2\sigma^2} (\theta^2 - 2\theta \bar{x} [ + \bar{x}^2 - \bar{x}^2]  + \frac{1}{N} \sum_i^N \hat{x}_i^2) 
	\\ &= -\frac{N}{2 \sigma^2} (\theta - \bar{x})^2 - \frac{N}{2 \sigma^2}(\frac{1}{N} \sum_{i=1}^{N} \hat{x}_i^2 - \bar{x}^2).
\end{align*}
So
\begin{align*}
	\mathcal{L}(\theta) &= \left( \frac{1}{\sqrt{2\pi}} \right)^N \exp \left(- \frac{N}{2 \sigma^2}(\frac{1}{N} \sum_{i=1}^{N} \hat{x}_i^2 - \bar{x}^2)\right) \exp \left(-\frac{N}{2 \sigma^2} (\theta - \bar{x})^2 \right)
	\\ &= L_0 \exp \left( -\frac{N}{2 \sigma^2} (\theta - \bar{x})^2 \right),
\end{align*}
where $L_0 = \left( \frac{1}{\sqrt{2\pi}} \right)^N \exp \left(- \frac{N}{2 \sigma^2}(\frac{1}{N} \sum_i \hat{x}_i^2 - \bar{x}^2)\right)$.

\section{Computing the Posterior Probability for Theta}
\label{sec:posterior}
Bayes theorem is given by:
\begin{align*}
	p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)}.
\end{align*}
Ignoring the normalization constant (as it is independent of $\theta$):
\begin{align*}
	p(\theta|x) \propto p(x|\theta)p(\theta).
\end{align*}
We have that
\begin{equation*}
	p(x|\theta) = L_0 \exp \left( -\frac{N}{2 \sigma^2} (\theta - \bar{x})^2 \right),
\end{equation*}
and
\begin{equation*}
	p(\theta) =  \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}\frac{\theta^2}{\Sigma^2}\right).
\end{equation*}

From these,
\begin{equation*}
	p(\theta|x) \propto \exp \left[ -\frac{1}{2} \left( \frac{(\theta - \bar{x})^2}{\sfrac{\sigma^2}{N}} +\frac{\theta^2}{\Sigma^2} \right) \right].
\end{equation*}

Taking just the exponent,
\begin{align*}
	\frac{(\theta - \bar{x})^2}{\sfrac{\sigma^2}{N}} + \frac{\theta^2}{\Sigma^2} &= \frac{N}{\Sigma^2\sigma^2} \left[\Sigma^2 (\theta - \bar{x})^2 + \frac{\sigma^2}{N} \theta^2 \right]
	\\ &= \left( \frac{1}{\Sigma^2} + \frac{N}{\sigma^2} \right) \left( \frac{1}{\sfrac{\sigma^2}{N} + \Sigma^2} \right) \left[ (\Sigma^2 + \frac{\sigma^2}{N})\theta^2 - 2\bar{x}\Sigma^2\theta + \Sigma^2\bar{x}^2 \right]
	\\ &= \left( \frac{1}{\Sigma^2} + \frac{N}{\sigma^2} \right) \left( \theta^2 - 2 \frac{\Sigma^2}{\sfrac{\sigma^2}{N} + \Sigma^2} \theta + \frac{\Sigma^2}{\sfrac{\sigma^2}{N} + \Sigma^2} \bar{x}^2 \right).
\end{align*}
The final term above is independent of $\theta$. Thus, as we are dealing with an exponent, we can subtract this term and add its square without losing proportionality. This allows us to complete the square so we have:
\begin{equation*}
	\left( \frac{1}{\Sigma^2} + \frac{N}{\sigma^2} \right) \left( \theta - \frac{\Sigma^2}{\sfrac{\sigma^2}{N} + \Sigma^2} \bar{x} \right)^2.
\end{equation*}

Putting this back inside the exponential we are left with:
\begin{equation*}
	p(\theta|x) \propto \exp \left[ -\frac{1}{2} \left( \frac{1}{\Sigma^2} + \frac{N}{\sigma^2} \right) \left( \theta - \frac{\Sigma^2}{\sfrac{\sigma^2}{N} + \Sigma^2} \bar{x} \right)^2 \right],
\end{equation*}

i.e. the posterior follows a Gaussian distribution with mean $\frac{\Sigma^2}{\sfrac{\sigma^2}{N} + \Sigma^2} \bar{x}$ and standard deviation $\left( \frac{1}{\Sigma^2} + \frac{N}{\sigma^2} \right)^{-\frac{1}{2}}$.

\section{Asymptotic Independence of Posterior on Prior}
For the posterior
\begin{equation*}
	p(\theta|x) \propto \exp \left[-\frac{1}{2} \frac{ \left( \theta - \frac{\Sigma^2}{\sfrac{\sigma^2}{N} + \Sigma^2} \bar{x} \right)^2}{ \left( \frac{1}{\Sigma^2} + \frac{N}{\sigma^2} \right)^{-1}} \right],
\end{equation*}
$
\begin{array}{l l l l}
	\text{as }N \rightarrow \infty, &\quad 
	\\ &\quad \left( \frac{1}{\Sigma^2} + \frac{N}{\sigma^2} \right)^{-1} &\rightarrow \frac{\sigma^2}{N} \quad &\left(\frac{N}{\sigma^2} \gg \frac{1}{\Sigma^2}\right), 
	\\ &\quad \frac{\Sigma^2}{\sfrac{\sigma^2}{N} + \Sigma^2} \bar{x} &\rightarrow \bar{x} \quad &\left(\frac{\sigma^2}{N} \rightarrow 0\right),
\end{array}
$
and the posterior becomes
\begin{equation*}
	p(\theta|x) \propto \exp \left[-\frac{1}{2} \frac{(\theta - \bar{x})^2}{\sfrac{\sigma^2}{N}} \right], 
\end{equation*}
i.e. the likelihood.

\section{Asymptotic Convergence of the Posterior Mean to the MLE Mean for Theta}
The posterior mean is given by:
\begin{equation*}
	\langle \theta \rangle = \int_{-\infty}^{+\infty} \theta p(\theta|x) \mathrm{d} \, \theta.
\end{equation*}
Inserting our equation for the posterior for $N \rightarrow \infty$, we have
\begin{equation*}
	\langle \theta \rangle = \int_{-\infty}^{+\infty} \theta \exp \left[- \frac{1}{2} \frac{(\theta - \bar{x})^2}{\sfrac{\sigma^2}{N}} \right] \mathrm{d} \, \theta. 
\end{equation*}
Making the substitution $y = \theta - \bar{x}$:
\begin{align*}
	\langle \theta \rangle &= \int_{-\infty}^{+\infty} (y+\bar{x})  \exp \left[- \frac{1}{2} \frac{y^2}{\sfrac{\sigma^2}{N}} \right] \mathrm{d} \, y
	\\ &= \int_{-\infty}^{+\infty} \bar{x} \exp \left(- \frac{1}{2}\frac{N}{\sigma^2} y^2 \right) \mathrm{d} \, y,
\end{align*}
as $ y \exp \left(- \frac{1}{2}\frac{N}{\sigma^2} y^2 \right) $ is an odd function.

Substituting back:
\begin{align*}
	\int_{-\infty}^{+\infty} \bar{x} \exp \left[- \frac{N}{\sigma^2} (\theta - \bar{x})^2 \right] \mathrm{d} \, \theta &= \bar{x} \int_{-\infty}^{+\infty} p(\theta|x) \mathrm{d} \, \theta
	\\ &= \bar{x},
\end{align*}
as the integral of a pdf between infinite limits is equal to one. 

\section{2-D Likelihood in Gaussian Form}
\label{sec:2dlikelihood}

For the linear model
\begin{equation*}
	y = F\theta + \epsilon
\end{equation*}
where $\epsilon$ is uncorrelated, the likelihood function is given by
\begin{equation*}
	p(y|\theta) = \frac{1}{(2\pi)^{\frac{d}{2}}\Pi_j\tau_j} \exp\left[-\frac{1}{2}(b - A\theta)^t(b-A\theta)\right],
\end{equation*}
where $A_{ij} = \sfrac{F_{ij}}{\tau_i}$ and $b_i = \sfrac{y_i}{\tau_i}$.

Taking just the variable part of the exponent,
\begin{align*}
	(b - A\theta)^t(b-A\theta) &= (A\theta - b)^t(A\theta - b)
	\\ &= (\theta - A^{-1}b)^tA^tA(\theta - A^{-1}b)
	\\ &= (b - AA^{-1}(A^t)^{-1}A^tb)^t(b - AA^{-1}(A^t)^{-1}A^tb) \, + \\ & \qquad (\theta - A^{-1} (A^t)^{-1} A^t b)^t A^t A(\theta - A^{-1} (A^t)^{-1} A^t b)
	\\ &= (b - A L^{-1} A^t b)^t (b - A L^{-1} A^t b) + (\theta - L^{-1} A^t b)^t L (\theta - L^{-1} A^t b)
	\\ &= (b - A \theta_0)^t (b - A \theta_0) + (\theta - \theta_0)^t L (\theta - \theta_0).
\end{align*}

Putting this back into the exponent above we have
\begin{align*}
	p(y|\theta) &= \frac{1}{(2\pi)^{\frac{d}{2}}\Pi_j\tau_j} \exp\left[-\frac{1}{2}(b - A \theta_0)^t (b - A \theta_0) -\frac{1}{2}(\theta - \theta_0)^t L (\theta - \theta_0)\right],
	\\ &= \frac{1}{(2\pi)^{\frac{d}{2}}\Pi_j\tau_j} \exp\left[-\frac{1}{2}(b - A \theta_0)^t (b - A \theta_0)\right] \exp\left[-\frac{1}{2} (\theta - \theta_0)^t L (\theta - \theta_0)\right].
	\\ &= \mathcal{L}_0 \exp\left[-\frac{1}{2} (\theta - \theta_0)^t L (\theta - \theta_0)\right].
\end{align*}

Where $\mathcal{L}_0 = \frac{1}{(2\pi)^{\frac{d}{2}}\Pi_j\tau_j} \exp\left[-\frac{1}{2}(b - A \theta_0)^t (b - A \theta_0)\right]$, $\theta_0 = L^{-1} A^t b$ and $L \equiv A^tA$.

\section{2-D Posterior in Gaussian Form}
\label{sec:2dposterior}

If the prior probability distribution function goes as
\begin{equation*}
	p(\theta) \propto \exp \left[ -\frac{1}{2} \theta^t P \theta \right],
\end{equation*}
where $P$ is the prior Fisher information matrix, and the likelihood function goes as
\begin{equation*}
	p(y|\theta) \propto \exp\left[-\frac{1}{2} (\theta - \theta_0)^t L (\theta - \theta_0)\right],
\end{equation*}
then according to Bayes theorem the posterior follows
\begin{align*}
	p(\theta|y) & \propto p(\theta) p(y|\theta) 
	\\ & \propto \exp \left[ -\frac{1}{2} \theta^t P \theta \right] \exp\left[-\frac{1}{2} (\theta - \theta_0)^t L (\theta - \theta_0)\right].
\end{align*}

Combining the exponents and taking the variable part of the result,
\begin{align*}
	\theta^t P \theta + (\theta - \theta_0)^t L (\theta - \theta_0) &= \theta^t P \theta + \theta^t L \theta - \theta_0^t L \theta - \theta^t L \theta_0 + \theta_0^t L \theta_0
	\\ &= \theta^t(L+P) \theta - \theta_0^t L \theta - \theta^t L \theta_0 + \theta_0^t L \theta_0.
\end{align*}
Constants can be added and subtracted from the exponent without effecting the proportionality, so we subtract $\theta_0^t L \theta_0$ and add $\theta_0^t L (L+P)^{-1} L \theta_0$:
\begin{align*}
	\theta^t(L+P) \theta - & \theta_0^t L \theta - \theta^t L \theta_0 + \theta_0^t L (L+P)^{-1} L \theta_0
	\\ &= \theta^t(L+P) \theta - \theta_0^t L \theta - \theta^t(L+P)(L+P)^{-1} L \theta_0 + \theta_0^t L (L+P)^{-1} L \theta_0
	\\ &= (\theta^t(L+P) - \theta_0^t L)(\theta - (L+P)^{-1} L \theta_0)
	\\ &= (\theta^t - \theta_0^t L (L+P)^{-1})(L+P)(\theta - (L+P)^{-1} L \theta_0)
\end{align*}
$P$ and $L$ are both fisher matrices, i.e. inverse covariance matrices. As
covariance matrices are symmetric, it follows that the inverse of the sum of
these two matrices is symmetric, and so $(L+P)^{-1} = ((L+P)^{-1})^{t}$. Thus we have
\begin{align*}
	(\theta^t - \theta_0^t L^t ((L+ & P)^{-1})^{t}) (L+P) (\theta - (L+P)^{-1} L \theta_0)
	\\ &= (\theta -(L+P)^{-1} L \theta_0)^{t} (L+P) (\theta - (L+P)^{-1} L \theta_0)
	\\ &= (\theta - \bar{\theta})^t \mathcal{F} (\theta - \bar{\theta}),
\end{align*}
where $\mathcal{F} = L + P$ and $\bar{\theta} = \mathcal{F}^{-1} L \theta_0$.
This, when put back into the exponent, gives the form of a multivariate
Gaussian:
\begin{equation*}
	p(\theta|y) \propto \exp \left[-\frac{1}{2}(\theta - \bar{\theta})^t \mathcal{F} (\theta - \bar{\theta})\right],
\end{equation*}
with mean $\bar{\theta}$ and Fisher matrix $\mathcal{F}$.

\section{Data}
\label{sec:2ddata}

\begin{tabular}{|c|c|}
	\hline
	x & y \\ 
	\hline 
	0.8308 & 0.9160 \\
	0.5853 & 0.7958 \\
	0.5497 & 0.8219 \\
	0.9172 & 1.3757 \\
	0.2858 & 0.4191 \\
	0.7572 & 0.9759 \\
	0.7537 & 0.9455 \\
	0.3804 & 0.3871 \\
	0.5678 & 0.7239 \\
	0.0759 & 0.0964 \\
	\hline
\end{tabular}

	


\section{onedcode} 
\label{sec:onedcode}

\section{twodcode}
\label{sec:twodcode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thebibliography}{10}


	\bibitem{metropolis}
		Metropolis et al. (1953) Equation of State Calculations by Fast
		Computing Machines. The Journal of Chemical Physics.
		[Online] 21 (6), 1087-1092. Available from: doi:
		10.1063/1.1699114 [Accessed 21 October 2013].

	\bibitem{applications}
		Diaconis, P., 2009. The Markov Chain Monte Carlo Revolution.
		Bull. Amer. Math. Soc., Vol. 46, pp. 179-205

	\bibitem{handbookch1}
		Geyer, C., J., 2011. Handbook of Markov Chain Monte Carlo.
		Chapman \& Hall/CRC, p.3
		
	\bibitem{halton}
		Halton, J., H., 1970. A Retrospective and Prospective Survey of
		the Monte Method. SIAM Rev., Vol. 12, No. 1, pp. 1-63
	
	\bibitem{som}
		Trotta, R., 2012. Statistics of Measurement: Summary Handout.
		Imperial College London, p. 14

\end{thebibliography}

\end{document}
